# @TODO(mattjmcnaughton) I should be more specific about whether I want this
# prometheus chart to be generic. If so, `rules-blog.yaml` shouldn't exist as a
# template, rather it should be specified via values.yaml.
#
# This manifest defines the PrometheusRules specific to our blog that we'll use
# for monitoring/alerting for mattjmcnaughton.com. Each rule should be
# self-explanatory, but we utilize the following conventions:
#
# - `severity=critical` indicates we should raise an alert via Opsgenie.
# - 15m is the default `for`. We may refine this value over time.
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: blog
  labels:
    app.kubernetes.io/name: blog
    app.kubernetes.io/environment: {{ .Values.environment }}
    helm.sh/chart: {{ include "prometheus.chart" . }}
{{ toYaml .Values.ruleSelectorLabels | indent 4 }}
spec:
  groups:
  - name: blog.rules
    rules:
      - alert: BlogDown
        annotations:
          message: 'Unable to scrape metrics for blog. Is blog down?'
        expr: 'absent(up{service="blog"}) == 1'
        # Make `for` more aggressive, because there's less chance of this
        # correcting itself.
        for: 5m
        # @TODO(mattjmcnaugthon) Determine how I want to share label values with
        # the `alertmanager` chart.
        labels:
          severity: critical
      - alert: SLOLatencyFailure
        annotations:
          message: 'Violating SLO that 99% latency is < 1s'
        expr: 'histogram_quantile(0.99, sum(rate(caddy_http_request_duration_seconds_bucket{service="blog"}[1h])) by (le)) > 1'
        for: 15m
        labels:
          severity: critical
      - alert: SLOAvailabilityFailure
        annotations:
          message: 'Violating SLO that 99% of requests are successful'
        expr: 'sum(rate(caddy_http_response_status_count_total{service="blog",status!~"5.."}[1h])) / sum(rate(caddy_http_response_status_count_total{service="blog"}[1h])) < .99'
        for: 15m
        labels:
          severity: critical
